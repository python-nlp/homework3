{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# [Your name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1b1765621be5b7fed8647e98abb74aa",
     "grade": false,
     "grade_id": "cell-5dec1ceda34753f6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 3\n",
    "\n",
    "The maximum score of this homework is 100+20 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However successful test do not guarantee that your solution is correct.\n",
    "The homework is partially autograded using many hidden tests.\n",
    "Test cells cannot be modified and empty cells cannot be deleted.\n",
    "\n",
    "Your solution should replace placeholder lines such as:\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "You don't need to write separate functions except when the function header is predefined.\n",
    "Variable names must be derived from the public test.\n",
    "Please do not add new cells, they will be ignored by the autograder.\n",
    "\n",
    "**VERY IMPORTANT** Before submitting your solution (pushing to the git repo),\n",
    "run your notebook with `Kernel -> Restart & Run All` and make sure it\n",
    "runs without exceptions.\n",
    "\n",
    "If your code fails the public tests (the ones you see), you will automatically receive 0 points for that exercise. Furthermore, later tasks within the same exercise might require the output of earlier ones; failing to provide implementation for the earlier tasks in this scenario will **prevent you from earning any points** for the later ones.\n",
    "\n",
    "## Submission\n",
    "\n",
    "GitHub Classroom will accept your last pushed version before the deadline.\n",
    "You do not need to send the homework to the instructor.\n",
    "\n",
    "## Plagiarism\n",
    "\n",
    "When preparing their homework, students are reminded to pay special attention to Title 32, Sections 92-93 of Code of Studies (quoted below). Any content from external sources must be stated in the students own words AND accompanied by citations. Copying and pasting from an external source should be avoided and any text copied must be placed between quotation marks. Reports that violate these rules cannot receive a passing grade.\n",
    "\n",
    "\"**Section 92**\n",
    "\n",
    "(1) The works of another person will be used as follows: a) if a work of another person is used in whole or in part (e.g. by copying, citation, translation from another language or presentation), the source and the name of the author will be indicated if this name is included in the source or – in case of orally presented works – may be clearly identified; b) the work of another person or any part of that will be used – up to a quantity reasonably corresponding to the nature and purpose of the student work – identified as quotations.\n",
    "\n",
    "(2) Instructors are entitled to review compliance with requirements in this article with computer programmes and databases.\n",
    "\n",
    "(3) The use of works of another person and the acknowledgement of use will be governed by applicable laws and the relevant rules of the specific discipline.\n",
    "\n",
    "**Section 93**\n",
    "\n",
    "(1) If a student fails to meet rules regarding use of works of another person in whole or in part, the student work will be considered as not assessable and the student will not be allowed to obtain the credit of the concerned subject in the specific term.\n",
    "\n",
    "(2) It will be deemed a disciplinary offence if a student – in breach of the rules regarding use of works of another person – submits or presents a work of another person fully or in a significant part verbatim (word for word) or in terms of its basic concepts or the combined version of several works of another person(s) as their own work.\n",
    "\n",
    "(3) Based on subsection (1) of Section 52/A. of the Higher Education Act, compliance with the rules regarding the use of works of another person in a master thesis may be reviewed up to five years following the issue of the degree certificate. In case of violation of the above rules, section 52/A of the Higher Education Act will apply.\"\n",
    "\n",
    "(BME Code of Studies, p.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e35e4df1b7e04b250fd1fec461c23911",
     "grade": false,
     "grade_id": "cell-d542629642130aa5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d07cbb5ee21f22467e4a9fada61be8e",
     "grade": false,
     "grade_id": "cell-845d83788b399620",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "import types\n",
    "import urllib.request\n",
    "\n",
    "import nltk\n",
    "from nltk import Nonterminal\n",
    "from nltk.tree import Tree\n",
    "import numpy\n",
    "\n",
    "def execute_commands(*cmds, fancy=True):\n",
    "    \"\"\"\n",
    "    Starts foma end executes the specified commands.\n",
    "    Might not work if there are too many...\n",
    "    \"\"\"\n",
    "    if fancy:\n",
    "        print('Executing commands...\\n=====================\\n')\n",
    "    args = ' '.join('-e \"{}\"'.format(cmd) for cmd in cmds)\n",
    "    output = subprocess.check_output('foma {} -s'.format(args),\n",
    "                                     stderr=subprocess.STDOUT,\n",
    "                                     shell=True).decode('utf-8')\n",
    "    print(output)\n",
    "    if fancy:\n",
    "        print('=====================\\n')\n",
    "    \n",
    "def compile_lexc(lexc_string, fst_file):\n",
    "    \"\"\"\n",
    "    Compiles a string describing a lexc lexicon with foma. The FST\n",
    "    is written to fst_file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='wt', encoding='utf-8', delete=False) as outf:\n",
    "            outf.write(lexc_string)\n",
    "    try:\n",
    "        execute_commands('read lexc {}'.format(outf.name),\n",
    "                         'save stack {}'.format(fst_file), fancy=False)\n",
    "        #!foma -e \"read lexc {outf.name}\" -e \"save stack {fst_file}\" -s\n",
    "    finally:\n",
    "        os.remove(outf.name)\n",
    "        \n",
    "def apply(fst_file, words, up=True):\n",
    "    \"\"\"\n",
    "    Applies the FST in fst_file on the supplied words. The default direction\n",
    "    is up.\n",
    "    \"\"\"\n",
    "    if isinstance(words, list):\n",
    "        words = '\\n'.join(map(str, words))\n",
    "    elif not isinstance(words, str):\n",
    "        raise ValueError('words must be a str or list')\n",
    "    invert = '-i' if not up else ''\n",
    "    result = subprocess.check_output('flookup {} {}'.format(invert, fst_file),\n",
    "                                     stderr=subprocess.STDOUT, shell=True,\n",
    "                                     input=words.encode('utf-8'))\n",
    "    words = result.decode('utf-8').strip().split('\\n\\n')  # Skip last newline\n",
    "    return [word.split('\\t') for word in words]\n",
    "       \n",
    "apply_up = partial(apply, up=True)\n",
    "apply_down = partial(apply, up=False)\n",
    "\n",
    "def draw_all_trees(parser, sentence):\n",
    "    for tree in parser.parse(sentence):\n",
    "        display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3e1e86fdd7fa75044ac022d601121df",
     "grade": false,
     "grade_id": "cell-699e60dd77fd2bf0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Although not necessary, feel free to copy any additional boilerplate code you need from labs [9](../../course_material/09_Morphology_lab/09_Morphology_lab.ipynb#Morphology) and [10](../../course_material/10_Syntax/10_Syntax_lab.ipynb#Boilerplate) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f3fe0800ab38fecd0bf88337fb1c0b7",
     "grade": false,
     "grade_id": "cell-3f095f2f07cf1178",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Morphology (33 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1960a481849aecf2c83e790970b6376b",
     "grade": false,
     "grade_id": "cell-9d7fe2068946a03e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1.1 Esperanto nouns (13 points)\n",
    "\n",
    "In this exercise, you are going to write a simple foma grammar for Esperanto nouns. Esperanto, like Hungarian, is agglutinative; however, as you will see, the similarities stop there. Your grammar will not cover all the ins and outs of Esperanto noun inflection, but it will give you a \"feel\".\n",
    "\n",
    "What follows are the rules of Esperanto noun declension. Your task is to write a `lexc` grammar based on them.\n",
    "\n",
    "1. All words begin with a root, such as the animal names below (`best` just means _animal_)\n",
    "   ```\n",
    "   hund\n",
    "   kat\n",
    "   bird\n",
    "   elefant\n",
    "   tigr\n",
    "   best\n",
    "   leon\n",
    "   ```\n",
    "1. The nominative is formed by the _o_ suffix, e.g. _hundo_.\n",
    "1. Three suffixes can be placed between the root and the _o_:\n",
    "    - the feminine marker _in_: e.g. _hundino_ (bitch)\n",
    "    - the diminutive marker _et_: e.g. _hundeto_ (puppy)\n",
    "    - the augmentative marker _eg_: e.g. hundego (big dog)\n",
    "1. There is no rule on the order or the number of these suffixes in a word. Make sure your grammar accepts everything from _hundo_ to _hundinetinegegeto_ (i.e. do not worry about the overgeneration).\n",
    "1. The plural is marked by the _j_ suffix, which follows the _o_; e.g. _hundoj_\n",
    "1. The accusative is marked by _n_; it is the last morpheme, e.g. in _hundojn_\n",
    "\n",
    "Note: start simple, and try to make the test cells work one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "79e05f0389aa08edb5438962c4ac42b0",
     "grade": false,
     "grade_id": "cell-65ca203d5e54e43b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d59b514b214acb17a23976f876965be",
     "grade": false,
     "grade_id": "cell-e6083dc8672c4c88",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_compile(grammar, fst_file):\n",
    "    try:\n",
    "        if os.path.exists(fst_file):\n",
    "            os.unlink(fst_file)\n",
    "        compile_lexc(grammar, fst_file)\n",
    "        assert True\n",
    "    except Exception as e:\n",
    "        assert False, 'Got exception: {}'.format(e)\n",
    "    \n",
    "def words_valid(fst_file, words, up=True):\n",
    "    for word in apply(fst_file, words, up):\n",
    "        if word[1] == '+?':\n",
    "            assert False, 'Word {} could not be parsed'.format(word[0])\n",
    "    assert True\n",
    "    \n",
    "def words_invalid(fst_file, words, up=True):\n",
    "    for word in apply(fst_file, words, up):\n",
    "        if word[1] != '+?':\n",
    "            assert False, 'Word {} should not have been parsed'.format(word[0])\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bdb91b92892bd541cad8570a70d8f138",
     "grade": true,
     "grade_id": "cell-c23dd80d44effbfa",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test nominative\n",
    "test_compile(esperanto_1, 'esperanto_1.fst')\n",
    "words_valid('esperanto_1.fst', ['hundo', 'kato', 'birdo', 'elefanto'])\n",
    "words_invalid('esperanto_1.fst', ['hund', 'kata', 'bird', 'elefantoo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1bc8bccd4a62da2d8757ee32e2169393",
     "grade": true,
     "grade_id": "cell-2dbc8215fb1dee10",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test plural\n",
    "words_valid('esperanto_1.fst', ['hundoj', 'tigroj', 'leono', 'bestoj'])\n",
    "words_invalid('esperanto_1.fst', ['hundj', 'tigr', 'leonojj', 'bestjo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a313bd0b068c756ce159b6c921e81d2",
     "grade": true,
     "grade_id": "cell-4216f032023916a9",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test accusative\n",
    "words_valid('esperanto_1.fst', ['hundon', 'katojn', 'birdon'])\n",
    "words_invalid('esperanto_1.fst', ['hundn', 'katonj', 'birdno'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f8618f331d8ddef956e5fd9ec9a8252",
     "grade": true,
     "grade_id": "cell-c2cae3ba16e01ae8",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test overgeneration\n",
    "words_valid('esperanto_1.fst', ['elefantegojn', 'leonino', 'bestetino', 'hundinetinegegeto'])\n",
    "words_invalid('esperanto_1.fst', ['elefantojegn', 'leonin', 'bestoetin', 'eghundinetinegeto'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 Male and female (5 points)\n",
    "\n",
    "Esperanto animal names can take the _ge_ prefix. It means \"male and female\" (i.e. a pair of the animal in question). Add the prefix to your grammar. Examples are _getigregoj_ and _geelefantojn_. Also, make sure that:\n",
    "- such nouns should always be plural (_j_), so no _gehundo_\n",
    "- the _in_ suffix is forbidden, so _geleoninoj_ is invalid\n",
    "\n",
    "The easiest way to do this would be with _flag diacritics_. This time, the `@U@` flag will probably not work; try to form an if-else structure with some of the others (e.g. `@P@` and `@D@`).\n",
    "\n",
    "Note:\n",
    "- make sure that the new grammar still accepts and rejects the same words as before if they don't start with _ge_\n",
    "- completion of this exercise is **not** required to solve 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "3ade2adac2ddc23945668c1d840c9ff3",
     "grade": false,
     "grade_id": "cell-cd8a7eb8d75e2d84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "290e3c2eae5a543269608619443e8688",
     "grade": true,
     "grade_id": "cell-9b217f1fac5bd037",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_compile(esperanto_2, 'esperanto_2.fst')\n",
    "words_valid('esperanto_2.fst', ['gehundoj', 'gekatetegoj', 'gebirdetojn'])\n",
    "words_valid('esperanto_1.fst', ['hundojn', 'tigro', 'leonino', 'elefantegetinegineto'])\n",
    "words_invalid('esperanto_2.fst', ['geelefanto', 'getigrinoj'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6781a97f915f1021fc02807d4f8c568b",
     "grade": false,
     "grade_id": "cell-f474ef302e23c80d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1.3 With tags (15 points)\n",
    "\n",
    "It is time now to convert your Esperanto FSA to a full-fledged morphological analyzer FST. Incorporate the following tags into the grammar:\n",
    "\n",
    "| Tag | Morpheme | Comment |\n",
    "|-----|----------|---------|\n",
    "| `MF+` | male-female | if implemented |\n",
    "| `+Noun` | noun roots | all of them... |\n",
    "| `+Fem` | feminine (_in_) | |\n",
    "| `+Dim` | diminutive (_et_) | |\n",
    "| `+Aug` | augmentative (_eg_) | |\n",
    "| `+NSuff` | the _o_ suffix | |\n",
    "| `+Pl` | plural (_j_) | |\n",
    "| `+Sg` | singular | when not _j_ |\n",
    "| `+Acc` | accusative (_n_) | |\n",
    "| `+Nom` | nominative | when not _n_ |\n",
    "\n",
    "The tags should **only** appear on the upper side; the morphemes (with the exception of roots) on the lower side. Some examples:\n",
    "\n",
    "| Lexical (upper) | Surface (lower) |\n",
    "|-----------------|-----------------|\n",
    "| `hund+Noun+NSuff+Sg+Nom` | _hundo_ |\n",
    "| `MF+best+Noun+Dim+NSuff+Pl+Acc` | _gebestetojn_ |\n",
    "\n",
    "Note: you don't have to complete 1.2 to earn points for this exercise. Also, don't forget: \"up\" is left, \"down\" is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "222dbc8bca7a71ece7acf703e866fd8f",
     "grade": false,
     "grade_id": "cell-4add29a1e97f171d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d78951209318c8b882dffcecc0185a70",
     "grade": true,
     "grade_id": "cell-b68e62d5c8fa24c3",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_analysis(fst_file, input, output, up=True):\n",
    "    res = apply(fst_file, [input], up)\n",
    "    assert res[0][1] == output, '{} is not parsed as {} but {}'.format(input, output, res[0][1])\n",
    "\n",
    "test_compile(esperanto_3, 'esperanto_3.fst')\n",
    "\n",
    "test_analysis('esperanto_3.fst', 'hundo', 'hund+Noun+NSuff+Sg+Nom')\n",
    "test_analysis('esperanto_3.fst', 'best+Noun+Dim+NSuff+Pl+Acc', 'bestetojn', False)\n",
    "test_analysis('esperanto_3.fst', 'birdetegetegeteginoj', 'bird+Noun+Dim+Aug+Dim+Aug+Dim+Aug+Fem+NSuff+Pl+Nom')\n",
    "\n",
    "words_valid('esperanto_3.fst', ['hund+Noun+NSuff+Pl+Acc', 'tigr+Noun+NSuff+Sg+Nom',\n",
    "                                'leon+Noun+Fem+NSuff+Sg+Nom',\n",
    "                                'elefant+Noun+Aug+Dim+Fem+Aug+Fem+Dim+NSuff+Sg+Nom'], False)\n",
    "words_invalid('esperanto_3.fst', ['kat+Noun+NSuff+NSuff+Sg+Acc', 'bird+Noun+Dim+NSuff+Acc+Acc',\n",
    "                                  'tigr+Noun+Pl+Nom', 'best+Acc', 'leon+NSuff+Dim+Sg+Nom'], False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2eadf4de6f5385c9dd7beca0600fc8f0",
     "grade": true,
     "grade_id": "cell-424ff45fd908717e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_analysis('esperanto_3.fst', 'gehundoj', 'MF+hund+Noun+NSuff+Pl+Nom')\n",
    "test_analysis('esperanto_3.fst', 'gebirdetegojn', 'MF+bird+Noun+Dim+Aug+NSuff+Pl+Acc')\n",
    "test_analysis('esperanto_3.fst', 'MF+best+Noun+Aug+NSuff+Pl+Acc', 'gebestegojn', False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02d22ff2aa9751586f5a11f52d8b060d",
     "grade": false,
     "grade_id": "cell-29561365ff642b53",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Toki Pona (47 points)\n",
    "\n",
    "[Toki Pona](https://en.wikipedia.org/wiki/Toki_Pona) is a constructed language aimed at simplicity. It was designed to \"shape the thought process of its users in a Zen-like fashion\". In this exercise, your task will be to write a phrase structure grammar for Toki Pona.\n",
    "\n",
    "Read the Wikipedia article linked above to familiarize yourself with the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c81b5acbf9fbd710ca5df8bb7bdc440",
     "grade": false,
     "grade_id": "cell-cb3ee1e110540933",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.1 Obtain the word list (10 points)\n",
    "\n",
    "Toki Pona has (around) 120 root words, each describing a simple concept. These can be compounded (arranged into noun phrases) to express more complex ideas. Root words are not sorted into part-of-speech classes; instead, a root word can be a noun, verb or modifier based on where it occurs in the sentence.\n",
    "\n",
    "Your first task is to get hold of the word list. The [canonical list](http://tokipona.net/tp/ClassicWordList.aspx) can be found on tokipona.net in HTML. Write a function that, given a URL, downloads the page and extracts the word list from it.\n",
    "\n",
    "Your functions should also have a `remove_stop` argument. Read the [Syntax part of the Wikipedia page](https://en.wikipedia.org/wiki/Toki_Pona#Syntax) to see which words are used only to separate parts of the sentence from one another. This is almost everything in the syntax rules description that is written in **bold**, with the exception of _interjections_ and `mi` and `sina`, `lon` and `tawa`. If `remove_stop` is `True`, these words should be removed from the returned list (i.e. only content words are should be returned).\n",
    "\n",
    "**Note:**: Do not use any third-party libraries for this; they might not be available to the grading environment. your code should work for all URL types; use the [urllib.request](https://docs.python.org/3/library/urllib.request.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "3657b7f0b6a40c1d8c627b577506ad8d",
     "grade": false,
     "grade_id": "cell-882cd5a1841087a1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_tp_roots(url, remove_stop=False):\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f23c141dd38515a81c9e5db10757cf4",
     "grade": true,
     "grade_id": "cell-ce23f0027d8ad257",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "roots = get_tp_roots('http://tokipona.net/tp/ClassicWordList.aspx')\n",
    "\n",
    "assert isinstance(roots, list) or isinstance(roots, set)\n",
    "assert len(roots) == 125\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b700b60830079ad3ed9d752892371fe1",
     "grade": true,
     "grade_id": "cell-e956a670f2c6beaa",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "content_roots = get_tp_roots('http://tokipona.net/tp/ClassicWordList.aspx', True)\n",
    "assert len(content_roots) == 114\n",
    "assert 'mi' in content_roots\n",
    "assert 'la' not in content_roots\n",
    "assert 'lon' in content_roots\n",
    "assert 'taso' not in content_roots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8167bfc15079f2176a0391a25d6c098c",
     "grade": false,
     "grade_id": "cell-4a7037ab58530915",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.2 Grammaticality (12 points)\n",
    "\n",
    "We will need functions to be able to test the grammar. Implement the following two functions:\n",
    "- `is_grammatical` accepts a parser and a sentence, and check whether the sentence is grammatical\n",
    "- `find_all_nt` accepts a `Tree` and a nonterminal (as a string). If returns a list of sentence fragments that are covered by the specified nonterminal. Study the [`Tree` API](http://www.nltk.org/_modules/nltk/tree.html#Tree) to find the function you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "b0fe159ab0bac195216ec42fe0856ba3",
     "grade": false,
     "grade_id": "cell-ae31c733d48367d1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1314091140ebcfc69d93b11f4b2f6e55",
     "grade": false,
     "grade_id": "cell-8707fb83ce48d96b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "test_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> \"n\"\n",
    "VP -> V NP | S\n",
    "V -> \"v\"\n",
    "\"\"\")\n",
    "test_parser = nltk.ChartParser(test_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "707a325feeeedb4a0652082cde6c29d3",
     "grade": true,
     "grade_id": "cell-6f8df0da8153ae3f",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert is_grammatical(test_parser, 'nvn')\n",
    "assert is_grammatical(test_parser, 'nnvn')\n",
    "assert not is_grammatical(test_parser, 'vn')\n",
    "\n",
    "# assert is_grammatical(tp, 'mama pi mi mute o sina lon sewi kon'.split())\n",
    "# assert is_grammatical(tp, 'tenpo ali la sina jo e ma e wawa e pona'.split())\n",
    "# assert not is_grammatical(tp, 'toki toki toki'.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f165e6c0f2f7c9f98e5f2438c14b4fcb",
     "grade": true,
     "grade_id": "cell-8c8f95418a1ce192",
     "locked": true,
     "points": 8,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_all_nt(test_parser.parse_one('nvn'), 'S') == ['n v n']\n",
    "assert find_all_nt(test_parser.parse_one('nvn'), 'NP') == ['n'] * 2\n",
    "assert find_all_nt(test_parser.parse_one('nnnvn'), 'S') == ['n n n v n', 'n n v n', 'n v n']\n",
    "assert find_all_nt(test_parser.parse_one('nnnvn'), 'NP') == ['n'] * 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bf65bfba1f1126016737447b3cb72f22",
     "grade": false,
     "grade_id": "cell-c0e092e5620ec50e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.3 A Toki Pona PSG grammar (25 points)\n",
    "\n",
    "Following the [Syntax description](https://en.wikipedia.org/wiki/Toki_Pona#Syntax), write an PSG grammar for Toki Pona. Simply translate the ten rules to the format accepted by NLTK, with the following caveats:\n",
    "\n",
    "- omit 8c and 8d\n",
    "- the name of the nonterminals must be the same as in the grammar description, only in CamelCase (e.g. _Verbal_, _SubClause_)...\n",
    "- ...with the exception of phrases and the sentence itself, which should have abbreviated names:\n",
    "    - Sentence: _S_\n",
    "    - Noun phrase: _NP_\n",
    "    - Preposition phrase: _PP_\n",
    "    - Verb phrase: _VP_\n",
    "    - Simple noun phrase: _SNP_\n",
    "- handle both optionality (_[]_) and repetition (_*_) in the grammar\n",
    "- you might want to introduce nonterminals aside from the ones in the grammar description. You can do so, but always make sure their names end with an underscore (`_`), so that tree-comparison tests work\n",
    "- don't forget to add the content roots to the grammar\n",
    "\n",
    "You don't have to write the whole grammar at the same time. The task can be divided in many ways; one division is presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1d02120b7cc7ec10073fcd05e71c640",
     "grade": false,
     "grade_id": "cell-1db1ee6138231a19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.1 Interjections\n",
    "\n",
    "This grammar should implement 1a from the Wikipedia list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "350c70c0e29b7fa692a17d91dfb76e7c",
     "grade": false,
     "grade_id": "cell-35f0cfcf2cee573e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# You have to create inj_grammar_str, above\n",
    "tp_inj_grammar = nltk.CFG.fromstring(inj_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "052c1c7ae6a6fffa4e64e95f12dbe6fc",
     "grade": true,
     "grade_id": "cell-3149a97d48cdabc1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tp_inj = nltk.ChartParser(tp_inj_grammar)\n",
    "\n",
    "assert tp_inj\n",
    "assert tp_inj_grammar.start().symbol() == 'S'\n",
    "\n",
    "assert is_grammatical(tp_inj, 'a'.split())\n",
    "assert is_grammatical(tp_inj, 'jaki'.split())\n",
    "assert find_all_nt(tp_inj.parse_one('pakala'.split()), 'Interjection') == ['pakala']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11219dad704ccf7234fbfb7a21899eb3",
     "grade": false,
     "grade_id": "cell-49056f77b1648b83",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.2 Nouns\n",
    "\n",
    "The next three parts concern the most important units in the grammar: noun phrases. We start with nouns. As we have previously seen, all content roots can act as nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "b55d733d1edd198fbb59b68f27553476",
     "grade": false,
     "grade_id": "cell-ad031dd0f10630ba",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_noun_grammar = nltk.CFG.fromstring(noun_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "695b06898a8995aaeadff53052b5ec03",
     "grade": true,
     "grade_id": "cell-1dcb3cfbff7bba3f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tp_noun = nltk.ChartParser(tp_noun_grammar)\n",
    "\n",
    "assert tp_noun_grammar.start().symbol() == 'Noun'\n",
    "\n",
    "assert is_grammatical(tp_noun, 'akesi'.split())\n",
    "assert is_grammatical(tp_noun, 'jaki'.split())\n",
    "assert tp_noun.parse_one('monsi'.split()).pos()[0][1] == 'Noun'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eba9cc8e5248985b84ce388fa3954822",
     "grade": false,
     "grade_id": "cell-60cec6fd6b7200d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.3 Simple noun phrases\n",
    "\n",
    "Now that we know what our nouns are, we can put them into simple noun phrases. This concerns parts 6a and b from the Wikipedia page.\n",
    "\n",
    "Forget the `tp_noun_grammar` object from above, but incorporate `noun_grammar_str` into your new `snp_grammar_str`, below. Also, you must have to handle _modifiers_ as well $-$ again, all content roots can behave as modifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "c564d3f0713255921e2e4ef5d3d4cc82",
     "grade": false,
     "grade_id": "cell-9e4717a51facccf3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_snp_grammar = nltk.CFG.fromstring(snp_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c5b683e36c5801773da5dc3fc98fcdc",
     "grade": true,
     "grade_id": "cell-7027c783b1589d84",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "tp_snp = nltk.ChartParser(tp_snp_grammar)\n",
    "\n",
    "assert tp_snp_grammar.start().symbol() == 'SNP'\n",
    "\n",
    "# Nouns are SNPs\n",
    "assert is_grammatical(tp_snp, 'akesi'.split())\n",
    "assert is_grammatical(tp_snp, 'jaki'.split())\n",
    "assert tp_snp.parse_one('monsi'.split()).pos()[0][1] == 'Noun'\n",
    "\n",
    "# Noun + modifier\n",
    "assert is_grammatical(tp_snp, 'jan utala'.split())\n",
    "assert is_grammatical(tp_snp, 'kasi kule'.split())\n",
    "little_dog = tp_snp.parse_one('soweli lili'.split())\n",
    "assert [pos for word, pos in little_dog.pos()] == ['Noun', 'Modifier']\n",
    "\n",
    "# Noun + modifiers\n",
    "assert is_grammatical(tp_snp, random.sample(content_roots, k=5))\n",
    "little_dog_eating = tp_snp.parse_one('soweli lili moku'.split())\n",
    "assert find_all_nt(little_dog_eating, 'Noun') == ['soweli']\n",
    "assert find_all_nt(little_dog_eating, 'Modifier') == ['lili', 'moku']\n",
    "\n",
    "# pi\n",
    "assert is_grammatical(tp_snp, 'mama pi mi mute'.split())\n",
    "tree = tp_snp.parse_one('poki kule pi soweli lili moku'.split())\n",
    "assert find_all_nt(tree, 'Noun') == ['poki', 'soweli']\n",
    "assert find_all_nt(tree, 'Modifier') == ['kule', 'lili', 'moku']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90d06c136579b8ab7a3b5dfd5a82fdf6",
     "grade": false,
     "grade_id": "cell-c07583571350b07c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.4 Noun phrases\n",
    "\n",
    "This is just 6c on top of the SNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "f363b3cf657edbb3f7b4a8780bbf6725",
     "grade": false,
     "grade_id": "cell-85c6d807f243c61a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_np_grammar = nltk.CFG.fromstring(np_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "130eadcb767ffad9dace5ccc8501da2c",
     "grade": true,
     "grade_id": "cell-64c0e666b3ab587b",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tp_np = nltk.ChartParser(tp_np_grammar)\n",
    "\n",
    "assert tp_np_grammar.start().symbol() == 'NP'\n",
    "\n",
    "# SNPs are NPs\n",
    "assert is_grammatical(tp_np, 'akesi'.split())\n",
    "assert tp_np.parse_one('monsi'.split()).pos()[0][1] == 'Noun'\n",
    "little_dog = tp_np.parse_one('soweli lili'.split())\n",
    "assert [pos for word, pos in little_dog.pos()] == ['Noun', 'Modifier']\n",
    "assert is_grammatical(tp_np, random.sample(content_roots, k=5))\n",
    "tree = tp_np.parse_one('poki kule pi soweli lili moku'.split())\n",
    "assert find_all_nt(tree, 'Noun') == ['poki', 'soweli']\n",
    "assert find_all_nt(tree, 'Modifier') == ['kule', 'lili', 'moku']\n",
    "\n",
    "# Conjunction\n",
    "assert is_grammatical(tp_np, 'akesi anu poki kule pi soweli lili moku'.split())\n",
    "tree = tp_np.parse_one('soweli lili moku pi jan utala en poki kule pi soweli lili'.split())\n",
    "assert find_all_nt(tree, 'Noun') == ['soweli', 'jan', 'poki', 'soweli']\n",
    "assert find_all_nt(tree, 'SNP') == ['soweli lili moku pi jan utala', 'soweli lili moku',\n",
    "                                    'poki kule pi soweli lili', 'poki kule']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92eaa8e0215aff1b9f70d1f2391a9459",
     "grade": false,
     "grade_id": "cell-7414a20106e4c563",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.5 Subjects, objects, etc.\n",
    "\n",
    "With the NP at hand, you can now build some of the other structures in the grammar:\n",
    "- subjects (4)\n",
    "- direct objects (10)\n",
    "- vocatives (3)\n",
    "- prepositional phreses (7)\n",
    "\n",
    "We cannot test them in the same grammar, so we need to create four grammars this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "47669e601fa8513d185bddf365e66c40",
     "grade": false,
     "grade_id": "cell-b5cf94cfe3a9c9cd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_subj_grammar = nltk.CFG.fromstring(subj_grammar_str)\n",
    "tp_obj_grammar = nltk.CFG.fromstring(obj_grammar_str)\n",
    "tp_voc_grammar = nltk.CFG.fromstring(voc_grammar_str)\n",
    "tp_pp_grammar = nltk.CFG.fromstring(pp_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6659e227cb14220ab2aee55ab1a80322",
     "grade": true,
     "grade_id": "cell-76824afc93499405",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tp_subj = nltk.ChartParser(tp_subj_grammar)\n",
    "tp_obj = nltk.ChartParser(tp_obj_grammar)\n",
    "tp_voc = nltk.ChartParser(tp_voc_grammar)\n",
    "tp_pp = nltk.ChartParser(tp_pp_grammar)\n",
    "\n",
    "assert tp_subj_grammar.start().symbol() == 'Subject'\n",
    "assert tp_obj_grammar.start().symbol() == 'DirectObject'\n",
    "\n",
    "# Subjects\n",
    "assert is_grammatical(tp_subj, 'mi'.split())\n",
    "assert tp_subj.parse_one('monsi li'.split()).pos()[0][1] == 'Noun'\n",
    "tree = tp_subj.parse_one('poki kule pi soweli lili moku li'.split())\n",
    "assert find_all_nt(tree, 'NP') == ['poki kule pi soweli lili moku']\n",
    "assert find_all_nt(tree, 'Subject') == ['poki kule pi soweli lili moku li']\n",
    "\n",
    "# Direct objects\n",
    "assert is_grammatical(tp_obj, 'e poki kule pi soweli lili moku'.split())\n",
    "assert not is_grammatical(tp_obj, 'e akesi anu poki kule pi soweli lili moku'.split())\n",
    "tree = tp_obj.parse_one('e poki kule pi soweli lili moku pi jan utala'.split())\n",
    "assert find_all_nt(tree, 'SNP') == ['poki kule pi soweli lili moku pi jan utala',\n",
    "                                    'poki kule pi soweli lili moku', 'poki kule']\n",
    "assert tree.label() == 'DirectObject'\n",
    "\n",
    "# Vocatives\n",
    "tree = tp_voc.parse_one('akesi anu poki kule pi soweli lili moku o'.split())\n",
    "assert find_all_nt(tree, 'Vocative') == ['akesi anu poki kule pi soweli lili moku o']\n",
    "\n",
    "# Prepositional phrases\n",
    "assert tp_pp.parse_one('kepeken akesi en soweli lili'.split()).pos()[0][1] == 'Preposition'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afd3ef9cc9111310565ae378a5965e9b",
     "grade": false,
     "grade_id": "cell-2f3dc25956306bde",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.6 Verb phrases\n",
    "\n",
    "Verb phrases (8, 9) are a bit more hairy, but nothing earth-shattering. As mentioned before, skip 8c and d. Don't forget to append the `DirectObject` grammar string you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "2dd1a4c6c935361dec609720fa8de468",
     "grade": false,
     "grade_id": "cell-1996a29ebf852c8e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_vp_grammar = nltk.CFG.fromstring(vp_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d0c6e35f885b56816bbab710a5c1c9c",
     "grade": true,
     "grade_id": "cell-10508b4c77e1be50",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert tp_vp_grammar.start().symbol() == 'VP'\n",
    "\n",
    "tp_vp = nltk.ChartParser(tp_vp_grammar)\n",
    "\n",
    "assert tp_vp.parse_one('lape'.split()).pos()[0][1] == 'Verb'\n",
    "assert tp_vp.parse_one('lape pona'.split()).pos()[1][1] == 'Modifier'\n",
    "tree = tp_vp.parse_one('moku e moku seli'.split())\n",
    "assert find_all_nt(tree, 'DirectObject') == ['e moku seli']\n",
    "assert find_all_nt(tree, 'SNP') == ['moku seli']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "059f3cb74aff6b4d84ed1dd2779addc9",
     "grade": false,
     "grade_id": "cell-1fa53dd2801838cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.7 Prepositions\n",
    "\n",
    "Prepositions incorporate the verb and preposition phrases (5). Don't forget to name the conjunction nonterminal occuring here differently from the one you used in the NP grammar; otherwise, the two rules will conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "7c16021ee6f66026441ca8187147fc0f",
     "grade": false,
     "grade_id": "cell-a4dbce1b2717aea0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_pred_grammar = nltk.CFG.fromstring(pred_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c965cefcdaac49b7f341cf3994446c7e",
     "grade": true,
     "grade_id": "cell-ee3b4aa889ec026d",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert tp_pred_grammar.start().symbol() == 'Predicate'\n",
    "\n",
    "tp_pred = nltk.ChartParser(tp_pred_grammar)\n",
    "\n",
    "tree = tp_pred.parse_one('moku e moku seli'.split())\n",
    "assert find_all_nt(tree, 'Predicate') == find_all_nt(tree, 'VP') == ['moku e moku seli']\n",
    "\n",
    "tree = tp_pred.parse_one('soweli lili pi jan utala'.split())\n",
    "assert find_all_nt(tree, 'Predicate') == ['soweli lili pi jan utala']\n",
    "assert len(find_all_nt(tree, 'SNP')) == 2\n",
    "\n",
    "assert not is_grammatical(tp_pred, 'soweli lili en jan utala'.split())\n",
    "assert is_grammatical(tp_pred, 'soweli lili anu jan utala'.split())\n",
    "tree = tp_pred.parse_one('tomo suli li awen jaki'.split())\n",
    "assert find_all_nt(tree, 'Predicate') == ['tomo suli li awen jaki', 'tomo suli', 'awen jaki']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eed7ec8a3bec7b3053cae315c5597fe1",
     "grade": false,
     "grade_id": "cell-4b9e98329facc50f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.8 Grand slam\n",
    "\n",
    "Incorporate the rest into your grammar: sentences and sub-clauses (1 and 2). Don't forget to append the grammar strings you have created thus far (you might not need all, e.g. `np` is part of `pred`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "f08ea14cd140ac644ce5965985b15778",
     "grade": false,
     "grade_id": "cell-b7d6ec78d1f82f15",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tp_grammar = nltk.CFG.fromstring(tp_grammar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "22e8a366979734928f573c773c1f16a7",
     "grade": true,
     "grade_id": "cell-0a9375554885d493",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert tp_grammar.start().symbol() == 'S'\n",
    "\n",
    "tp = nltk.ChartParser(tp_grammar)\n",
    "\n",
    "trees = list(tp.parse('mama pi mi mute o sina lon sewi kon'.split()))\n",
    "assert len(trees) == 11\n",
    "assert ['mama', 'mi', 'sewi'] in [find_all_nt(t, 'Noun') for t in trees]\n",
    "assert ['lon'] in [find_all_nt(t, 'Verb') for t in trees]\n",
    "assert ['sina'] in [find_all_nt(t, 'Verb') for t in trees]\n",
    "\n",
    "trees = list(tp.parse('tenpo ali la sina jo e ma e wawa e pona'.split()))\n",
    "assert len(trees) == 1\n",
    "assert find_all_nt(trees[0], 'DirectObject') == ['e ma', 'e wawa', 'e pona']\n",
    "\n",
    "assert is_grammatical(tp, 'toki'.split())\n",
    "assert not is_grammatical(tp, 'toki toki'.split())\n",
    "assert is_grammatical(tp, 'jan meli li toki e toki pona'.split())\n",
    "assert is_grammatical(tp, 'soweli seme mu li utala ala'.split())\n",
    "assert is_grammatical(tp, 'mi mute li lape e lipu'.split())\n",
    "assert is_grammatical(tp, 'waso li olin e pan li tawa wawa lon kon'.split())\n",
    "assert is_grammatical(tp, 'jan lili laso li lon'.split())\n",
    "assert is_grammatical(tp, 'jan li tawa kepeken kepeken'.split())\n",
    "assert is_grammatical(tp, 'jan li tawa kepeken noka'.split())\n",
    "assert is_grammatical(tp, 'kute li lon'.split())\n",
    "assert not is_grammatical(tp, 'kute li'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d99d4ab11a1bb344c02fa6579d31766c",
     "grade": false,
     "grade_id": "cell-d613eeac9389cd76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.9 Grammar properties\n",
    "\n",
    "Compute the three numbers printed below. Lexical productions are those that have a single terminal (`str`) on the right hand side (i.e. the `A` $\\rightarrow$ `a` format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "477b6bba560410a164071e468494dacd",
     "grade": false,
     "grade_id": "cell-8f81029fcf13995c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "164b70abeea2bf900b2d5132c0417f90",
     "grade": true,
     "grade_id": "cell-c21284f175a860d2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Total number of productions:', num_all_productions)\n",
    "print('Number of lexical productions:', num_lex_productions)\n",
    "print('Number of nonterminals', num_nt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6400d10b4cdcd9a8bfc7d75547a0e683",
     "grade": false,
     "grade_id": "cell-fe38c065d2673e7a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3.10 Normalize your grammar\n",
    "\n",
    "If you created your grammar by concatenating separate grammar strings, you probably ended up with a lot of productions. The reason for this is that many rules were present in more than one grammar string. Normalize your grammar by making sure each line occurs only once in `tp_grammar_str`.\n",
    "\n",
    "Compute the three statistics for the normalized grammar and see how the number of productions goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "b921ba192e3c1ca5c4e95bd87ca5c1d4",
     "grade": false,
     "grade_id": "cell-ca98bfaf804f04a0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db87b793763c1da73f3e8c5bc44ca55e",
     "grade": true,
     "grade_id": "cell-7ec927080d7613ad",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Total number of productions (norm):', num_all_productions_norm)\n",
    "print('Number of lexical productions (norm):', num_lex_productions_norm)\n",
    "print('Number of nonterminals (norm)', num_nt_norm)\n",
    "\n",
    "assert num_nt == num_nt_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ccac0be8908b843c1f467a71d1feee96",
     "grade": false,
     "grade_id": "cell-ee0d4dc97b6dd6ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: Grammar induction (20 points)\n",
    "\n",
    "In this exercise, you will parse a treebank, and induce a PCFG grammar from it. Finally, you will use your grammar to compute the probability of the trees in the training corpus.\n",
    "\n",
    "You need a probabilistic version of the CKY algorithm to parse sentences with a PCFG. PCKY is not part of this homework; however, if you are interested, you can try to implement it based on the CKY code you created during the lab exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "10b32871283c92fd53fa486414b69ad3",
     "grade": false,
     "grade_id": "cell-fe084281cd2efd99",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1 Parse a treebank (10 points)\n",
    "\n",
    "Parse the treebank file `en_lines-ud-train.s` in the notebook's directory. Write a **generator** function that reads the file and yields `nltk.tree.Tree` objects. In particular,\n",
    "- do not read the whole file into memory\n",
    "- the `Tree.fromstring()` function converts an s-expression into a tree\n",
    "\n",
    "Open the file in an editor to see the formatting.\n",
    "\n",
    "Note that the file was created by parsing the [LinES dependency corpus](https://github.com/UniversalDependencies/UD_English-LinES/tree/master) with [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/), so it is not a gold standard by any means, but it will suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "da7d10714b0a74c79017494ad0299080",
     "grade": false,
     "grade_id": "cell-de934e28df16f1c3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7f0356e70a16e5c0af5c83c88e37d2d1",
     "grade": true,
     "grade_id": "cell-9b22d2290fd74614",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trees = parse_treebank('en_lines-ud-train.s')\n",
    "\n",
    "assert isinstance(trees, types.GeneratorType)\n",
    "assert sum(1 for _ in parse_treebank('en_lines-ud-train.s')) == 2613\n",
    "assert isinstance(next(parse_treebank('en_lines-ud-train.s')), Tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84e59a5bee3d6091c9d5a922ab02767e",
     "grade": false,
     "grade_id": "cell-1cf5199082dc12a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.2 Filter trees (5 points)\n",
    "\n",
    "In order to avoid problems further down the line, we shall only handle a subset of the trees in the treebank. We call a tree _valid_, if\n",
    "- its root is `'S'`\n",
    "- the root has at least two children.\n",
    "\n",
    "Write a function that returns `True` for \"valid\" trees and `False` for invalid ones. Filter the your generator with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "43fdc788b5d7a424694d126191f1c86f",
     "grade": false,
     "grade_id": "cell-f2ed1f864eef42dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a1b957d6f9cfae6d0a75a772a7709841",
     "grade": true,
     "grade_id": "cell-3d51b08f973d39c9",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "filtered_trees = filter(is_tree_valid, trees)\n",
    "\n",
    "assert sum(map(is_tree_valid, filtered_trees)) == 2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb90643494c6993034d0f4e59cf9d320",
     "grade": false,
     "grade_id": "cell-be4646b57fb107b3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.3 Induce the PCFG grammar (5 points)\n",
    "\n",
    "Now that you have the trees, it is time to induce (train) a PCFG grammar for it! Luckily, `nltk` has a functions for just that: [`nltk.grammar.induce_pcfg`](http://www.nltk.org/api/nltk.html#nltk.grammar.induce_pcfg). Use it to acquire your PCFG grammar. You can find hints at how to use it in the [grammar module](http://www.nltk.org/_modules/nltk/grammar.html).\n",
    "\n",
    "Note: since we want to parse sentences with the PCKY algorithm, we need our grammar to be in CNF. Unfortunately, `nltk` cannot convert a grammar to CNF, so you have to ensure that the trees are in CNF before feeding them to the PCFG induction function. That way, we can be sure that our grammar will be also. There are two functions that ensure a tree is in CNF:\n",
    "- [`collapse_unary`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.collapse_unary). Make sure you call it with `collapsePOS=True`!\n",
    "- [`chomsky_normal_form`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.chomsky_normal_form). Do not use any smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "54d59bbca1c9ec051692ed69c5dacda5",
     "grade": false,
     "grade_id": "cell-420327f08975167d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "50f9f43e4cdd63c855e637cf619d0bf2",
     "grade": true,
     "grade_id": "cell-c8f540312b59671b",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))\n",
    "\n",
    "assert len(grammar.productions()) == 15000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a34499dbfe2b1534a40849a9dfb21027",
     "grade": false,
     "grade_id": "cell-bd6e15ab360e95c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.4 *Bonus exercise: training corpus probability (10 points)\n",
    "\n",
    "Write a function that, given a PCFG and a parse tree, computes the probability of the latter. Be sure to use and return [log probability](https://en.wikipedia.org/wiki/Log_probability), otherwise the probabilities may become too small for `float` to manage. Probabilistic production objects support both the `prob` and the `logprob` methods.\n",
    "\n",
    "The trees shown to the function might not be in CNF, so make sure you convert them before computing the probability.\n",
    "\n",
    "Note: store production probabilities in a (global) dictionary to speed up your function. The second test cell will run very long if you use just regular `nltk` functions -- certainly too long for the autograder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "checksum": "c338a5776c8a0364cb2962dbb3e05b2c",
     "grade": false,
     "grade_id": "cell-3aa21184268a5d10",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f640cd80575e2613c27c4d63f0d8ff36",
     "grade": true,
     "grade_id": "cell-0bf0fa3a4ff07d69",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trees = list(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))\n",
    "\n",
    "assert numpy.allclose(tree_logprob(grammar, trees[0].copy(True)), -214.22993036407965)\n",
    "assert numpy.allclose(tree_logprob(grammar, trees[1000].copy(True)), -60.930244885517915)\n",
    "assert numpy.allclose(tree_logprob(grammar, trees[2000].copy(True)), -211.13780770649575)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0e2cc6f69ab56ff7ea03c8e7ea21e07",
     "grade": true,
     "grade_id": "cell-5f19c161f27bb3dd",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tree_logprobs = [tree_logprob(grammar, tree.copy(True)) for tree in trees]\n",
    "\n",
    "min_logprob = min(tree_logprobs)\n",
    "max_logprob = max(tree_logprobs)\n",
    "sum_logprob = sum(tree_logprobs)\n",
    "\n",
    "print('Min logprob:', min_logprob)\n",
    "print('Max logprob:', max_logprob)\n",
    "print('Corpus probability:', sum_logprob)\n",
    "\n",
    "assert numpy.allclose(min_logprob, -1050.314868185599)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
